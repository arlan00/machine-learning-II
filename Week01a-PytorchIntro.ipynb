{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1atugD1WGID9pKAvjNtbjXVgrGO8fbCja","timestamp":1771271175435}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dxdkVlGjZgQD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1771271237195,"user_tz":-60,"elapsed":9713,"user":{"displayName":"Csapo Adam","userId":"16958213297317009702"}},"outputId":"e4e89b82-c7d9-412b-fba0-8d95565f52cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"]}],"source":["!pip install numpy\n","!pip install torch"]},{"cell_type":"markdown","source":["# 1. What is a PyTorch tensor?\n","\n","The most basic data structure in PyTorch is the tensor"],"metadata":{"id":"LBs5Pmz6gnrf"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"MC_B34QJdXOr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten = torch.tensor([[[1], [2], [3]], [[4], [5], [6]]])"],"metadata":{"id":"aLuFicX9gvAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten"],"metadata":{"id":"WlTgjjyNhFBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To understand the shape of a tensor, read from the outside. In this example:\n","[[[1], [2],[3]], [[4], [5], [6]]]\n","- the outermost list has 2 items\n","- each of those 2 items has 3 items internally\n","- each of those 3 items has 1 item internally"],"metadata":{"id":"RhfaTCnSXf-y"}},{"cell_type":"code","source":["ten.shape"],"metadata":{"id":"jQFDvGeShFju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(ten.shape)"],"metadata":{"id":"Xs2Mx-cohH6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(ten.shape) [1]"],"metadata":{"id":"egMRCSZBz7L2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 1: Create a PyTorch Tensor and Check Its Shape**\n","Use the following nested list as your data:\n","[[[7], [8]], [[9], [10]], [[11], [12]]]\n","Create a PyTorch tensor from the given data and store it in a variable named myten.\n","\n","Display myten to confirm it was created correctly.\n","\n","Display the shape of myten.\n","\n","Convert the shape into a Python list using exactly this command (with the space):\n","list (myten.shape)\n","\n","From the shape list, display the third value (index [2]) using exactly this expression pattern:\n","list(myten.shape)[2]\n","\n","In one short sentence, explain what the shape means for this tensor.\n","\n"],"metadata":{"id":"MBAYIkRh1ykX"}},{"cell_type":"code","source":[],"metadata":{"id":"2CP5EQa3_9Bt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Reshaping with .view()\n","\n","This example demonstrates how tensor shapes work in PyTorch and how the view() method can reshape a tensor without changing its underlying data. First, we create a tensor and check its original shape. Then we reshape the same 24 values into different valid 2D layouts (3×8, 2×12, and 4×6). The goal is to understand that reshaping changes only the tensor’s dimensions, not the order or the content of the elements."],"metadata":{"id":"HnRVpVub-Nul"}},{"cell_type":"code","source":["ten=torch.tensor([[[1,2,3,4], [5,6,7,8], [9,10,11,12]], [[13,14,15,16],\n"," [17,18,19,20], [21,22,23,24]]], dtype=torch.float32)"],"metadata":{"id":"bputYT8fBfVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(ten.shape)"],"metadata":{"id":"M53mRfcCCHcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten.view(3,8)"],"metadata":{"id":"ZzLzmym_CRi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten.view(2,12)"],"metadata":{"id":"4y_YAz7bCZPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten.view(4,6)"],"metadata":{"id":"6XcM1qQuCm3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten.view(8,1,3)"],"metadata":{"id":"JUGjnCB0ZLAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Note: view() doesn't modify the tensor, so the original still has the same shape\n","ten.shape"],"metadata":{"id":"UykXavzjZSsJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 2: Reshape a Tensor with view() and Use -1**\n","\n","Use the following tensor data (keep the values exactly as given). Create a PyTorch tensor from this data and store it in a variable named myten2 (use dtype=torch.float32):\n","[[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24]]]\n","\n","Display myten2 to confirm it was created correctly.\n","\n","Display the shape of myten2 and convert it into a Python list using: list (myten2.shape).\n","\n","Reshape myten2 using view() and display the reshaped tensor each time:\n","(-1, 8)\n","(4, 2, 3)\n","\n","Try the following reshape and observe that it will fail (this line is only for understanding; it should raise an Exception):\n","(5, 5)\n","\n","Convert the shape of each successfully reshaped tensor into a Python list (use the same style as before, e.g., list ( ... .shape)).\n","\n","In one short sentence, explain why the (5, 5) reshape fails but the other reshapes work.\n","\n","Convert the shape of each successful reshaped tensor into a Python list (use the same style as before, e.g., list ( ... .shape)).\n","\n","In one short sentence, explain why the (5, 5) reshape fails but the other reshapes work."],"metadata":{"id":"sA1cfVz3clT7"}},{"cell_type":"code","source":[],"metadata":{"id":"vDkC_3G5c9DP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 2b: Reshape a Tensor with view()**\n","\n","Use the numbers 1–30 as your data (all values must be included exactly once). Create a PyTorch tensor from these values and store it in a variable named myten2.\n","\n","Display myten2 to confirm it was created correctly.\n","\n","Display the shape of myten2 and convert it into a Python list using: list (myten2.shape).\n","\n","Reshape myten2 into each of the following shapes using view() and display the reshaped tensor each time:\n","\n","(5, 6)\n","(3, 10)\n","(2, 15)\n","\n","Convert the shape of each reshaped tensor into a Python list (use the same style as before, e.g., list ( ... .shape)).\n","\n","Reshape myten2 using view(6, -1) and display the reshaped tensor. Then display its shape and convert it into a Python list using: list ( ... .shape).\n","\n","In one short sentence, explain what stays the same and what changes when you use view() on a tensor."],"metadata":{"id":"qvRW2elCJBH7"}},{"cell_type":"code","source":[],"metadata":{"id":"_WpXVsanN08Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. \"Wildcard\" dimension length of -1\n","\n","In view(), you can set one dimension to -1 and PyTorch will compute it automatically. Reshaping is only possible if the product of the new dimensions equals the total number of elements in the tensor.\n","This example demonstrates how PyTorch’s view() method can reshape a tensor using the special value -1, and why reshaping must always preserve the total number of elements.\n","Then we reshape the same tensor into a valid 3D layout with mytensor.view(3, 2, 4).\n","Finally, we try mytensor.view(3, 2, 3), which fails because the product of the requested dimensions does not match the tensor’s total number of elements.\n","\n","The goal is to understand that view() can only change the tensor’s shape when the element count remains the same, and that -1 can be used to let PyTorch compute one dimension automatically."],"metadata":{"id":"hBAVPz9JJIhP"}},{"cell_type":"code","source":["ten=torch.tensor([[[1,2,3,4], [5,6,7,8], [9,10,11,12]], [[13,14,15,16], [17,18,19,20], [21,22,23,24]]], dtype=torch.float32)"],"metadata":{"id":"OeuTFXKCfGwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten.view (-1,6)"],"metadata":{"id":"3g3K48YIfSk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten.view (3,2,4)"],"metadata":{"id":"D-yntL-5feWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ten.view (3,2,3)"],"metadata":{"id":"UI6Go5IXYwd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["myten3.view(4, 2, 3)"],"metadata":{"id":"ggEyHSLweAOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["myten.view(5, 5)  # this will raise an error (25 != 24)"],"metadata":{"id":"Thiu3AVOeDj9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 3: Reshape a Tensor with view() and use -1**\n","\n","Use the following tensor data (keep the values exactly as given). Create a PyTorch tensor from this data and store it in a variable named myten3:\n","\n","[[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24]]]\n","\n","Display myten3 to confirm it was created correctly.\n","\n","Display the shape of myten3 and convert it into a Python list using: list (myten3.shape).\n","\n","Reshape myten3 using view() and display the reshaped tensor each time:\n","\n","(-1, 8)\n","\n","(4, 2, 3)\n","\n","Try the following reshape and observe that it will fail (this line is only for understanding):\n","\n","(5, 5)\n","\n","Convert the shape of each successful reshaped tensor into a Python list (use the same style as before, e.g., list ( ... .shape)).\n","\n","In one short sentence, explain why the (5, 5) reshape fails but the other reshapes work.\n"],"metadata":{"id":"Ta-nL4pcgoWT"}},{"cell_type":"code","source":[],"metadata":{"id":"AoPUDrbnetg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Reshaping tensors with .squeeze()\n","\n","This example shows that squeeze() removes dimensions of size 1. To make this effect visible, the tensor is first reshaped with view(24, -1), which creates a shape like [24, 1] (PyTorch computes the -1 automatically). Then squeeze() removes that extra 1 dimension, resulting in a simpler 1D tensor with shape [24]."],"metadata":{"id":"47kUle2dXILL"}},{"cell_type":"code","source":["mytensor = torch.tensor([[1, 2, 3, 4, 5, 6],\n","                         [7, 8, 9, 10, 11, 12],\n","                         [13, 14, 15, 16, 17, 18],\n","                         [19, 20, 21, 22, 23, 24]], dtype=torch.float32)"],"metadata":{"id":"4v6BV51ghats"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor.shape"],"metadata":{"id":"_OZzjk2JhoeJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor.squeeze()#no effect bc more than 1 dimension has a length > 1"],"metadata":{"id":"VamdxwMLh3Du"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor.view(24, -1).squeeze ().shape"],"metadata":{"id":"IioCuFVRiNlA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor.view(24,-1).shape"],"metadata":{"id":"mDxXCkSMiiXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 4: Use view() and squeeze() to Remove a Size-1 Dimension**\n","\n","Use the following data (do not change the values). Create a PyTorch tensor from this data and store it in a variable named myten4 (use dtype=torch.float32):\n","\n","[[101, 102, 103, 104, 105], [106, 107, 108, 109, 110], [111, 112, 113, 114, 115]]\n","\n","Display myten4 to confirm it was created correctly.\n","\n","Display the shape of myten4 and convert it into a Python list using: list (myten4.shape).\n","\n","Reshape myten4 using view(15, -1) and display the shape of the reshaped tensor.\n","\n","Apply squeeze() to the reshaped tensor and display the shape again.\n","\n","Convert both shapes (before and after squeeze()) into Python lists using the same style as before (e.g., list ( ... .shape)).\n","\n","In one short sentence, explain what squeeze() removed and why it could remove it in this example."],"metadata":{"id":"HuLbfSeMlaEo"}},{"cell_type":"code","source":[],"metadata":{"id":"U46NMioCmJQF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Reshaping tensors with .unsqueeze()\n","\n","This example demonstrates that unsqueeze() is the opposite of squeeze(): it adds a new dimension of size 1, and you must specify where to insert it. After reshaping the tensor to have shape [1, 24], x.unsqueeze(1) inserts a new dimension in the middle to produce shape [1, 1, 24], while x.unsqueeze(2) inserts it at the end to produce shape [1, 24, 1]. The goal is to see that unsqueeze() changes only the shape, not the data."],"metadata":{"id":"IGmhljUNp2ri"}},{"cell_type":"code","source":["x = mytensor.view(-1, 24)"],"metadata":{"id":"SO9Epu3Dp4So"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"id":"l7kBwXiGqG3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape"],"metadata":{"id":"TwVaxzXaqRC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.unsqueeze(1)"],"metadata":{"id":"49WXeuKZqU2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Note that just like .view(), .unsqueeze() does not overwrite the original tensor\n","## Here, x still has the old shape\n","x.shape"],"metadata":{"id":"P_YBk4x4iBt3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.unsqueeze(2)"],"metadata":{"id":"7Yap8wGdqebq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.unsqueeze(2).shape"],"metadata":{"id":"oDePEjtXiOcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.unsqueeze(3)#dimension out of range"],"metadata":{"id":"14vKjW0Wq5VV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 5:**\n","\n","Use the following data (do not change the values). Create a PyTorch tensor from this data and store it in a variable named myten5:\n","\n","[[31, 32, 33, 34, 35], [36, 37, 38, 39, 40]]\n","\n","Display myten5 to confirm it was created correctly.\n","\n","Display the shape of myten5 and convert it into a Python list using: list (myten5.shape).\n","\n","Reshape myten5 using view(1, 10) and display the shape of the reshaped tensor.\n","\n","Apply unsqueeze(1) to the reshaped tensor and display the new shape.\n","\n","Apply unsqueeze(2) to the reshaped tensor and display the new shape.\n","\n","Convert each resulting shape into a Python list (use the same style as before, e.g., list ( ... .shape)).\n","\n","In one short sentence, explain the difference between unsqueeze(1) and unsqueeze(2) in this example."],"metadata":{"id":"vk950EgTuOjv"}},{"cell_type":"code","source":[],"metadata":{"id":"z4B5DtJHTgHt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. .unsqueeze() = [:, None, ...]\n","\n","This example shows an alternative to unsqueeze(): you can add a new dimension by using slicing with None (also called newaxis). The expression mytensor[:, None, :, :] inserts a size-1 dimension at a specific position, producing the same kind of shape change as mytensor.unsqueeze(1). The most important point is that the position of None in the indexing determines where the new dimension is inserted. The goal is to understand that both methods change only the tensor’s shape, not its data."],"metadata":{"id":"JExNcnrxxo7L"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"8uIRAclmVWCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor = torch.arange(1, 25).view(2, 3, 4).to(torch.float32)\n","mytensor"],"metadata":{"id":"HUjkHSEIVW5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor.shape"],"metadata":{"id":"BiJPfjkayNgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor.unsqueeze(1)"],"metadata":{"id":"r4KUxLCFyUe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor[:, None, :, :].shape"],"metadata":{"id":"TvNeWZQFWA7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor = torch.arange(1, 25).view(2, 3, 4)\n","mytensor[:, None, :, :].shape"],"metadata":{"id":"VwatrheOzFn8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 6: Add a New Dimension with None (Slicing) in Different Positions**\n","\n","Use the following data (do not change the values). Create a PyTorch tensor from this data and store it in a variable named myten6:\n","\n","[[201, 202, 203, 204], [205, 206, 207, 208], [209, 210, 211, 212]]\n","\n","Display myten6 to confirm it was created correctly.\n","\n","Display the shape of myten6 and convert it into a Python list using: list (myten6.shape).\n","\n","Create three new tensors using slicing with None (do not use unsqueeze() in this exercise). Display the shape of each result:\n","\n","Insert a new dimension at the beginning (before all existing dimensions).\n","\n","Insert a new dimension in the middle (between the first and second dimension).\n","\n","Insert a new dimension at the end (after the last dimension).\n","\n","Convert each resulting shape into a Python list using the same style as before (e.g., list ( ... .shape)).\n","\n","In one short sentence, explain how the position of None affects the resulting shape."],"metadata":{"id":"iODZaswY2YIM"}},{"cell_type":"code","source":[],"metadata":{"id":"SlUKKUfKY1Hn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. Slicing tensors, use of .item()\n","\n","This example demonstrates that PyTorch tensors can be indexed and sliced in a NumPy-like way. mytensor[0, 2, 1] selects a single element using three indices (one per dimension), while mytensor[0, 2, 1:3] uses a slice to select a range of elements from the last dimension. Finally, .item() converts a single-element tensor (a 0-dim tensor) into a standard Python scalar value, which is useful for printing or using the value outside PyTorch."],"metadata":{"id":"M4q-LaE02aOy"}},{"cell_type":"code","source":["mytensor"],"metadata":{"id":"DyOU3wBp3x0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor[0, 2, 1]"],"metadata":{"id":"YHbF1p5F32Ih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor[0, 2, 1:3]"],"metadata":{"id":"A0MqdNRP35SK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mytensor[0, 2, 1].item()"],"metadata":{"id":"QXNDooy638uk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 7: Indexing, Slicing, and .item() in PyTorch**\n","\n","Use the following data (do not change the values). Create a PyTorch tensor from this data and store it in a variable named myten_index:\n","\n","`[[[41, 42, 43, 44],\n","[45, 46, 47, 48],\n","[49, 50, 51, 52]],\n","\n","[[53, 54, 55, 56],\n","[57, 58, 59, 60],\n","[61, 62, 63, 64]]]`\n","\n","Tasks:\n","\n","Create the tensor exactly as given and store it in myten_index. Display the tensor.\n","\n","Select and display a single element at index [0, 2, 1].\n","\n","Select and display a slice at index [0, 2, 1:3].\n","\n","Convert the single element from step 2 into a Python scalar using .item() and display the scalar value.\n","Note: .item() works only if the result contains exactly one value.\n","\n","In one short sentence, explain the difference between selecting a single element and selecting a slice."],"metadata":{"id":"HMk12ouO9fqf"}},{"cell_type":"code","source":[],"metadata":{"id":"rJLnMd1Ob9Jd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. Other ways to create PyTorch tensors\n","\n","This example shows that PyTorch provides many convenient tensor constructors. torch.zeros(3, 2) creates a 3×2 tensor filled with zeros, torch.randn(3, 2) creates a 3×2 tensor with random values from a standard normal distribution\n","N(0,1), and 3 * torch.rand(3, 2) + 2 generates uniform random values in the [0, 1) range, and then scales and shifts them so the final numbers fall in the range [2,5)."],"metadata":{"id":"IAS4lPwODsDy"}},{"cell_type":"code","source":["torch.zeros(3, 2)"],"metadata":{"id":"vYv79x4TDwg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.randn(3, 2) #normal dist - N(0, 1) with specified size tensor"],"metadata":{"id":"nfaY1Ou4D517"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["3*torch.randn(3, 2) + 2 # uniform dist in range 2, 5 tensor"],"metadata":{"id":"E8u79eH7eHjv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 8: Tensor Constructors (zeros, randn, and scaling)**\n","\n","Create the following tensors in PyTorch and display each result:\n","\n","Create a 4×3 tensor filled with zeros.\n","\n","Create a 4×3 tensor with random values from a standard normal distribution\n","N(0,1).\n","\n","Create a 4×3 tensor with random values from N(0,1), then scale and shift it using the formula: 2 * (random_tensor) - 1\n","\n","In one short sentence, explain the difference between the three tensors you created."],"metadata":{"id":"_iOUwZ-BgJFv"}},{"cell_type":"code","source":[],"metadata":{"id":"S7ENhCOdgqgp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. Use of other types of random distributions\n","\n","This example demonstrates how to use PyTorch’s Poisson distribution to generate random samples and compute basic statistics and probabilities. It creates a Poisson distribution with a given rate parameter, draws samples as tensors (or as Python scalars using .item()), retrieves the variance, evaluates log-probabilities for specific values, and finally generates a batch of samples with a chosen output shape (e.g., [3, 2])."],"metadata":{"id":"5loStksskDfw"}},{"cell_type":"code","source":["from torch.distributions import poisson"],"metadata":{"id":"XAgZp0F4lVZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p = poisson.Poisson(4.0)"],"metadata":{"id":"trlJ_vaclZN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p.sample()"],"metadata":{"id":"1JIWVqGbloCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p.sample().item()"],"metadata":{"id":"_0Z8TmA4lubn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p.sample().item()"],"metadata":{"id":"d8E-sEPplzNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p.sample().item()"],"metadata":{"id":"EAu7Q4CNl1AC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p.variance.item()"],"metadata":{"id":"WJepv8I-l7h5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["-1 * p.log_prob(torch.tensor(4.)).item()"],"metadata":{"id":"X0zjYBSPmFzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["-1 * p.log_prob(torch.tensor(int(0.1))).item()"],"metadata":{"id":"YuOzcCgDnK1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p.sample((3, 2))"],"metadata":{"id":"NEKo6S7pnUnL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 9: Sampling and Log-Probabilities with a Poisson Distribution (PyTorch)**\n","\n","Create a Poisson distribution in PyTorch with rate = 6.0 and store it in a variable named p9.\n","Tasks:\n","Draw one sample from p9 and display it as a tensor.\n","Draw another sample and convert it to a Python scalar using .item() (display the scalar).\n","Display the variance of p9 as a Python scalar using .variance.item().\n","Compute and display the log-probability of k = 5 using log_prob(...) (return a Python scalar).\n","Compute and display the log-probability of k = 0 using log_prob(...) (return a Python scalar).\n","Draw a batch of samples with shape (2, 4) and display the result. Calculate the negative log probability of the whole batch.\n","\n","In one short sentence, explain what the rate parameter controls in a Poisson distribution."],"metadata":{"id":"5BLgUbBhocca"}},{"cell_type":"code","source":[],"metadata":{"id":"LAJEeHcjpVLL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Matrix multiplication using .mm()\n","\n","This example shows that matrix multiplication in PyTorch can be performed with mm() on 2D tensors, and that the inner dimensions must match. A 1D vector b is reshaped into a 2D column vector (via transpose) so it has shape [2, 1], making it compatible with a of shape [2, 2]. Then a.mm(b) computes the matrix–vector product and returns a [2, 1] result."],"metadata":{"id":"MczxF2P56__G"}},{"cell_type":"code","source":["a = torch.tensor([[1, 2], [3, 4]])"],"metadata":{"id":"xWZnxGEh7BGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b = torch.tensor([[5, 9]])"],"metadata":{"id":"1vg72s1d7y2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.shape"],"metadata":{"id":"_46-SaG6DSzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b.shape"],"metadata":{"id":"c3QWfW4WDYKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b=b.t()"],"metadata":{"id":"tEQ3u2O9Tj_g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b.shape"],"metadata":{"id":"EEruivJk9ECn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"id":"NdeJF4nD9h6j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.mm(b)"],"metadata":{"id":"MfFQE5-q79y3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 10: Matrix Multiplication with mm() and Shape Matching**\n","\n","Use the following values (do not change them):\n","\n","Matrix A: [[3, 1], [2, 5]]\n","\n","Vector b (start as a 1D tensor): [4, 7]\n","\n","Tasks:\n","- Create A as a 2D tensor and b as a 1D tensor.\n","- Display A.shape and b.shape.\n","- Reshape b into a 2D column vector with shape [2, 1].\n","- Compute A.mm(b_column) and display the result.\n","- Display the final result shape."],"metadata":{"id":"WfUiZ8xxAMG8"}},{"cell_type":"code","source":[],"metadata":{"id":"z6UNNzjCAwmo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 11. Multiplying tensors via temporary reshaping\n","\n","Example: Matrix Multiplication with a 3D Tensor (Temporary Reshape)\n","In this example, you learn what to do when a tensor has more than 2 dimensions, but you still want to use matrix multiplication (mm()), which works only with 2D tensors.\n","\n","We start with a tensor x that is 3D (shape [4, 3, 2]). Each “row” in the last dimension contains 2 values, like [1, 2], [3, 4], etc.\n","We also create a small column vector y with shape [2, 1].\n","\n","Because mm() requires 2D tensors, we temporarily reshape x into a 2D tensor with shape [-1, 2]. This keeps the same values in the same order in memory—only the shape changes.\n","After multiplication, we reshape the result back to a structured form (shape [4, 3, 1]) so it matches the original 3D layout.\n","\n","Key idea: Reshaping changes only the tensor’s shape, not the order of elements, so the last dimension still contains the pairs of values we expect during multiplication."],"metadata":{"id":"Ie96gzvTUlkr"}},{"cell_type":"code","source":["x = torch.tensor([[[1.0, 2],   [3, 4],   [5, 6]], [[7, 8],   [9, 10],  [11, 12]], [[13, 14], [15, 16], [17, 18]], [[19, 20], [21, 22], [23, 24]]])"],"metadata":{"id":"0cbfjyUWVVaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape"],"metadata":{"id":"QtZcfYP5VkRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = torch.tensor([[2, 3]], dtype=torch.float32).t()"],"metadata":{"id":"DXCCn8GjVpEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.shape"],"metadata":{"id":"73vP_OF0VtIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"id":"B5flfZN8Vwmj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"_7bGJnMYV1IL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x.shape)\n","print(x.dtype)\n","print(y.shape)\n","print(y.dtype)\n","print(x.view(-1,2).shape)\n","new_var = x.view(-1,2).to(torch.float32).mm(y)\n","#new_var = x.to(torch.float32).view(-1, 2).mm(y)\n","new_var"],"metadata":{"id":"dnEMqc-kXFig"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 11: Matrix Multiplication with a 3D Tensor**\n","\n","This exercise shows a common beginner mistake: matrix multiplication fails if the two tensors have different dtypes (e.g., long vs float). You will first run a “wrong” version that produces the error, then fix it by converting the dtypes to match.\n","\n","Data (use exactly these values):\n","\n","3D tensor x with shape [2, 2, 2]:\n","[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n","\n","Column vector y (shape [2, 1]):\n","[[1.5], [2.0]]\n","\n","Tasks:\n","\n","Create x and y exactly as given.\n","\n","Try to compute x.view(-1, 2).mm(y) (this should fail).\n","\n","Fix the dtype mismatch and run the multiplication again.\n","\n","Reshape the result back to shape [2, 2, 1]."],"metadata":{"id":"kgf3lBsdCbva"}},{"cell_type":"code","source":[],"metadata":{"id":"kgzLZoujV8uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Expected error message (or very similar):RuntimeError: expected m1 and m2 to have the same dtype, but got: long int != float\n","# --- Step 4: FIX (make dtypes match) ---\n","# Option A (recommended): convert x to float\n","x_fix = x.to(torch.float32)\n","\n","# Now the multiplication works\n","out = x_fix.view(-1, 2).mm(y)\n","out\n","\n","# --- Step 5: Reshape back to the structured 3D layout [2, 2, 1] ---\n","out_3d = out.reshape(2, 2, 1)\n","out_3d, out_3d.shape\n"],"metadata":{"id":"DEd8M8jCDrkA"}},{"cell_type":"markdown","source":["# 12. Element-wise multiplication\n","\n","This example shows that tensors can be multiplied in two different ways in PyTorch. First, it demonstrates matrix multiplication with mm(), which combines rows and columns and follows the rules of linear algebra (works for 2D tensors). Then it demonstrates element-wise multiplication with * (or torch.mul()), which multiplies values position by position and works for tensors of any dimension as long as their shapes are compatible. The 3D example confirms that element-wise multiplication keeps the same shape when the input shapes match."],"metadata":{"id":"Is3q047ZFgBC"}},{"cell_type":"code","source":["a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)"],"metadata":{"id":"Ui3zMOrUF8Q3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","b = torch.tensor([[1, 2], [5, 3]], dtype=torch.float32)"],"metadata":{"id":"WtCwUj-JGJow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"id":"El4abwu6GPpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b"],"metadata":{"id":"8dNkBBz5GUiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.mm(b)"],"metadata":{"id":"WoTrkJbOGVh-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a * b"],"metadata":{"id":"17DfXVlZGaXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.mul(a, b)"],"metadata":{"id":"Q5GZsQCuIy4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([[[1]], [[3]]], dtype=torch.float32)"],"metadata":{"id":"78TwRUz5GeIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = torch.tensor([[[6]], [[2]]], dtype=torch.float32)"],"metadata":{"id":"nalHTBG2Grv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape"],"metadata":{"id":"Z3p6s4VgGzFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.shape"],"metadata":{"id":"9caBbOaJG2l5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x * y"],"metadata":{"id":"TrRpk2YoG8ym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(x * y).shape"],"metadata":{"id":"875fcpt4I63T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.mul(x, y)"],"metadata":{"id":"QpghS8hCI_KO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.mul(x, y).shape"],"metadata":{"id":"HVh060OFJDFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 12: Matrix Multiplication vs. Element-wise Multiplication\n","Use the following values exactly.**\n","\n","Part A (2D tensors)\n","\n","Create two 2D tensors:\n","\n","A = [[2, 0], [1, 3]]\n","\n","B = [[4, 1], [2, 5]]\n","\n","Tasks:\n","\n","Create tensors A and B (use dtype=torch.float32) and display them.\n","\n","Compute and display the matrix multiplication A.mm(B).\n","\n","Compute and display the element-wise multiplication A * B.\n","\n","Compute and display the same element-wise result using torch.mul(A, B).\n","\n","Part B (3D tensors)\n","\n","Create two 3D tensors:\n","\n","X = [[[2]], [[5]]]\n","\n","Y = [[[3]], [[4]]]\n","\n","Tasks:\n","5. Create tensors X and Y (use dtype=torch.float32) and display X.shape and Y.shape.\n","6. Compute and display the element-wise multiplication X * Y.\n","7. Display the shape of the result using both:\n","\n","(X * Y).shape\n","\n","torch.mul(X, Y).shape\n","\n","In one short sentence: explain the difference between mm() and * for tensors."],"metadata":{"id":"3NLsQEfLG7nr"}},{"cell_type":"code","source":[],"metadata":{"id":"QVaxABENMtRN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 13. Shape mismatches aren't always the end of the world. Broadcasting is an important mechanism in PyTorch\n","\n","This example demonstrates broadcasting in PyTorch during element-wise multiplication. Even if two tensors have a different number of dimensions, the operation can still work if their shapes are compatible: PyTorch automatically “expands” the smaller tensor (as needed) to match the larger tensor’s shape. Here, y.squeeze() reduces y to a 1D tensor, but x.mul(y) still works because PyTorch broadcasts y across the extra size-1 dimensions of x. The example also explains why multiplying a tensor by a scalar (e.g., x * 3) will apply the operation to every element."],"metadata":{"id":"msBRvJe2RcgY"}},{"cell_type":"code","source":["x = torch.tensor([[[1]], [[3]]], dtype=torch.float32)"],"metadata":{"id":"3dsazqMKR1t_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"id":"MrJTm42TR4Xh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape"],"metadata":{"id":"TGyp-rL6SVnc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = torch.tensor([[[6]], [[2]]], dtype=torch.float32)"],"metadata":{"id":"SP7G1EoRSZvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.shape"],"metadata":{"id":"tNRfJXCGrpij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_squeezed = y.squeeze()"],"metadata":{"id":"f0tt14V3SfV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_squeezed"],"metadata":{"id":"B5_mBJwzSixR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_squeezed.shape"],"metadata":{"id":"WvFFf-hmSmYd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Here are the criteria for broadcasting:**\n","\n","When operating on two tensors:\n","\n","1. Align shapes from the rightmost dimension\n","\n","2. For each aligned dimension:\n","\n","  - The sizes must either be:\n","\n","    - equal, or\n","\n","    - one of them is 1\n","\n","3. Missing leading dimensions are treated as size 1\n","\n","If all dimensions satisfy this rule → broadcasting works."],"metadata":{"id":"c7ho0yzasgXS"}},{"cell_type":"code","source":["## in this example:\n","## shapes aligned from the right:\n","## x -         (2, 1, 1)\n","## ysqueezed -       (2,)\n","## w/ missing dims:\n","##.            (1, 1, 2)\n","## -> Great! All the aligned dimension lengths either match or one of them is len 1\n","\n","z = x.mul(y_squeezed)\n","z"],"metadata":{"id":"uVKt0HfqSs7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z.shape"],"metadata":{"id":"H-wfmUicrxdw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x * 3"],"metadata":{"id":"IjnyMYOWSwqe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 13: Broadcasting in Element-wise Multiplication (with squeeze())**\n","\n","This exercise practices broadcasting in PyTorch: element-wise multiplication can still work even if two tensors have different numbers of dimensions, as long as their shapes are compatible.\n","\n","Use the following values exactly.\n","\n","Data\n","\n","Create a 3D tensor x7 (shape [2, 1, 1]):\n","[[[2]], [[4]]]\n","\n","Create a 3D tensor y7 (shape [2, 1, 1]):\n","[[[5]], [[3]]]\n","\n","Tasks\n","\n","Import PyTorch.\n","\n","Create x7 and display it and its shape.\n","\n","Create y7, then create y7_squeezed = y7.squeeze(). Display y7_squeezed and its shape.\n","\n","Compute and display x7.mul(y7_squeezed) (this should work because of broadcasting).\n","\n","Multiply x7 by the scalar -2 and display the result.\n","\n","In one short sentence: explain why x7.mul(y7_squeezed) works even though the tensors have different numbers of dimensions."],"metadata":{"id":"ENLanxEdS_1M"}},{"cell_type":"code","source":[],"metadata":{"id":"NqgNueFCT0-c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 14. Moving Tensors to the GPU (CUDA) for Faster Computation\n","\n","This example demonstrates how to place PyTorch tensors on the GPU to speed up computations. First, it checks the tensor’s type on the CPU. Then it moves the tensor to the GPU using .cuda() (only available if a CUDA-capable GPU is present). Finally, it shows the recommended, portable approach: select a device (\"cuda\" if available, otherwise \"cpu\") and move tensors to that device using .to(device)."],"metadata":{"id":"toz1WFqcUpN_"}},{"cell_type":"code","source":["a = torch.tensor([[1, 2], [3, 4]])"],"metadata":{"id":"ucmWg-KFXiMy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"id":"bPRIEEkzXoEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.device"],"metadata":{"id":"ag9zfahJZatd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.dtype"],"metadata":{"id":"64tKUqCgZblu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a.type()"],"metadata":{"id":"df7quqTIXpDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"id":"cdXz4v0OXssV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"9wIQLCGfZmr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b = a.to(device)"],"metadata":{"id":"Mo4Wkx7RZqMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b"],"metadata":{"id":"iLGa9EPlZtSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b.device"],"metadata":{"id":"nvaTgQ1EZyGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b.dtype"],"metadata":{"id":"lxKU4YsCZy9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b.type()"],"metadata":{"id":"M7lNYiSOZ3Uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 14: Move a Tensor to GPU (CUDA) if Available**\n","\n","This exercise practices how to check CUDA availability, choose a device, and move a tensor to the selected device using .to(device).\n","\n","Data\n","\n","Use this matrix exactly: [[10, 20, 30], [40, 50, 60]]\n","\n","Tasks\n","\n","Import PyTorch.\n","\n","Create a tensor named t1 from the data above (use dtype=torch.float32) and display it.\n","\n","Print t1.device and t1.dtype.\n","\n","Check whether CUDA is available using torch.cuda.is_available() and display the result.\n","\n","Create a variable named device using:\n","torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","Move t1 to the selected device and store it in t2. Display t2.\n","\n","Print t2.device and t2.dtype.\n","\n","Theory question (one short sentence):\n","Why do we check torch.cuda.is_available() before using CUDA?"],"metadata":{"id":"rJ2oaYa8adnW"}},{"cell_type":"code","source":[],"metadata":{"id":"DR0zzxiQbHq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 15. Automatic Differentiation (Autograd) and Gradient Accumulation in PyTorch\n","\n","This example demonstrates how PyTorch can automatically compute derivatives using autograd. We treat a tensor w as a learnable variable by setting requires_grad=True, then define an expression\n","y=a⋅w**2+a⋅w\n","Calling y.backward() computes the derivative dy/dw and stores it in w.grad. The example also shows that gradients accumulate by default: calling backward() twice adds the new gradient on top of the old one, so we must reset (zero) the gradient before recomputing it.\n","\n","However, do note that before calling backward() again, we must re-compute the graph because everything gets freed when calling backward()"],"metadata":{"id":"upd656WGb_P9"}},{"cell_type":"code","source":["torch.__version__"],"metadata":{"id":"OPch9oAkbvez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.tensor([10.0])"],"metadata":{"id":"jfs0nmYpkdyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w = torch.tensor([2.0], requires_grad=True)"],"metadata":{"id":"J9ylrzTVkhiV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a, w"],"metadata":{"id":"hm1kfOCZkncd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = a * w**2 + a * w"],"metadata":{"id":"PfaK6xDkkymq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"TSzjC9tCk1tG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w.grad"],"metadata":{"id":"NnG5tAIok2dQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.backward() # derivative w.r.t. w: 2aw + a = 50"],"metadata":{"id":"_68cThpjk7w0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w.grad"],"metadata":{"id":"Cn6X2ITTk-j-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = a * w**2 + a * w # must re-compute the graph!!\n","y.backward()"],"metadata":{"id":"b91eQk5klB9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w.grad"],"metadata":{"id":"VRTzXMpmlFF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w.grad.zero_()"],"metadata":{"id":"YGsB4OHblIs0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = a * w**2 + a * w"],"metadata":{"id":"nvLY6o6VlL7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y.backward()"],"metadata":{"id":"x_Tg_H8klOCf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w.grad"],"metadata":{"id":"Gdo3CC5mlR6F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 15: Autograd and Gradient Accumulation in PyTorch**\n","\n","This exercise practices how PyTorch computes derivatives automatically using autograd, how gradients are stored in .grad, and why gradients must be reset before recomputing them.\n","\n","Tasks\n","\n","Import PyTorch.\n","\n","Create two tensors:\n","\n","a1 = 4.0 (no gradient needed)\n","\n","w1 = 3.0 with requires_grad=True\n","\n","Define the function:\n","y1 = a1 * w1**2 + a1 * w1\n","\n","Print w1.grad before calling backward().\n","\n","Call y1.backward() and print w1.grad.\n","\n","Call y1.backward() a second time and print w1.grad again (observe what happens).\n","\n","Reset the gradient with w1.grad.zero_().\n","\n","Recompute y1 (same formula) and call backward() again. Print w1.grad.\n","\n","Theory question (one short sentence):\n","Why does w1.grad become larger after calling backward() twice?"],"metadata":{"id":"vHm6a18fmEn5"}},{"cell_type":"code","source":[],"metadata":{"id":"jzN7oRXKmTRt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 16. Build a Simple Feed-Forward Neural Network Manually (Focus on Shapes)\n","\n","This example puts earlier concepts together and shows how a simple feed-forward (FF) neural network can be built manually using matrix multiplication. The goal is not training yet, but to understand how tensor shapes must match at each step.\n","\n","You will:\n","\n","choose a device (GPU if available, otherwise CPU),\n","\n","create a small input dataset and target values,\n","\n","initialize weights and biases for a 2-layer network,\n","\n","compute the forward pass step by step,\n","\n","print the shapes to verify that every matrix multiplication and bias addition is valid.\n","\n","Key idea: matrix multiplication requires matching inner dimensions, and biases must be broadcastable to the layer output shape."],"metadata":{"id":"k3dOV7_AnCww"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"olO_TJqOzLTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) Choose device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)"],"metadata":{"id":"nUglPhe9zPRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2) Training data (5 samples, 2 features) and targets (5 samples, 1 output)\n","training_inputs = torch.tensor(\n","    [[1, 0],\n","     [1, 1],\n","     [0, 0],\n","     [0, 1],\n","     [1, 0]],\n","    dtype=torch.float32\n",").to(device)\n","\n","training_targets = torch.tensor(\n","    [[1],\n","     [0],\n","     [0],\n","     [1],\n","     [1]],\n","    dtype=torch.float32\n",").to(device)\n","\n","print(\"Input dimensions:\", repr(training_inputs.shape))\n","print(\"Target dimensions:\", repr(training_targets.shape))"],"metadata":{"id":"hIvzC4fAzWbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3) Define network sizes\n","input_dim = training_inputs.shape[1]   # 2\n","hidden_dim = 1                         # keep it simple\n","output_dim = 1                         # binary output (one value)\n"],"metadata":{"id":"WTet5OIWzgZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4) Initialize weights and biases\n","# W1: (input_dim, hidden_dim) so inputs.mm(W1) -> (N, hidden_dim)\n","W1 = torch.randn(input_dim, hidden_dim, device=device)\n","b1 = torch.randn(hidden_dim, device=device)   # broadcastable bias\n","\n","# W2: (hidden_dim, output_dim) so hidden.mm(W2) -> (N, output_dim)\n","W2 = torch.randn(hidden_dim, output_dim, device=device)\n","b2 = torch.randn(output_dim, device=device)   # broadcastable bias\n","\n","print(\"\\nShapes:\")\n","print(\"W1 shape:\", repr(W1.shape))\n","print(\"b1 shape:\", repr(b1.shape))\n","print(\"W2 shape:\", repr(W2.shape))\n","print(\"b2 shape:\", repr(b2.shape))"],"metadata":{"id":"fQ8PhQ2ozlUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5) Forward pass (no training yet)\n","# Layer 1: linear\n","z1 = training_inputs.mm(W1) + b1\n","print(\"\\nShape of (inputs.mm(W1)):\", repr(training_inputs.mm(W1).shape))\n","print(\"Shape after adding b1:\", repr(z1.shape))"],"metadata":{"id":"L1CAafCRzmuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Activation (optional, but typical in FF networks)\n","h1 = torch.sigmoid(z1)\n","print(\"Shape after sigmoid:\", repr(h1.shape))\n"],"metadata":{"id":"KLh_WYjIzs2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Layer 2: linear\n","z2 = h1.mm(W2) + b2\n","print(\"\\nShape of (h1.mm(W2)):\", repr(h1.mm(W2).shape))\n","print(\"Shape after adding b2:\", repr(z2.shape))"],"metadata":{"id":"3s5WmBFUzxAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Final output (e.g., probability-like output)\n","y_pred = torch.sigmoid(z2)\n","print(\"Final output shape:\", repr(y_pred.shape))"],"metadata":{"id":"YqXdMo5Sz1Q6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6) (Optional) Show the output values\n","print(\"\\nPredictions:\\n\", y_pred)\n","\n","input(\"\\nPress Enter to finish...\")"],"metadata":{"id":"ZwsQD3KVz570"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 16: Manual Forward Pass of a Tiny Feed-Forward Network (Check Shapes)**\n","\n","In this exercise you will build a very small 2-layer feed-forward network manually (no torch.nn yet). The focus is to practice shape matching for mm() and bias broadcasting.\n","\n","Data (use exactly these values):\n","\n","Inputs (6 samples, 3 features):\n","\n","[[1, 0, 1],\n"," [0, 1, 1],\n"," [1, 1, 0],\n"," [0, 0, 1],\n"," [1, 0, 0],\n"," [0, 1, 0]]\n","\n","\n","Targets (6 samples, 1 output):\n","\n","[[1], [0], [1], [0], [1], [0]]\n","\n","\n","Network sizes:\n","\n","input_dim = 3\n","\n","hidden_dim = 2\n","\n","output_dim = 1\n","\n","Tasks\n","\n","Choose a device (cuda if available, else cpu).\n","\n","Create training_inputs and training_targets on the device (dtype=torch.float32).\n","\n","Initialize:\n","\n","W1 with shape (input_dim, hidden_dim)\n","\n","b1 with shape (1, hidden_dim) (broadcastable)\n","\n","W2 with shape (hidden_dim, output_dim)\n","\n","b2 with shape (1, output_dim) (broadcastable)\n","\n","Compute a forward pass:\n","\n","z1 = training_inputs.mm(W1) + b1\n","\n","h1 = torch.sigmoid(z1)\n","\n","z2 = h1.mm(W2) + b2\n","\n","y_pred = torch.sigmoid(z2)\n","\n","Print the shapes of training_inputs, W1, z1, h1, W2, z2, and y_pred to verify everything matches.\n","\n","Print y_pred.\n","\n","Theory question (one short sentence):\n","Why do we use b1 with shape (1, hidden_dim) instead of (6, hidden_dim)?"],"metadata":{"id":"00FjqrzY0cZK"}},{"cell_type":"code","source":[],"metadata":{"id":"1dLEukft1YVJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 17. Manual Training Step with Autograd\n","\n","This example demonstrates a fully manual training step for a tiny 2-layer feed-forward network in PyTorch without using torch.nn or optimizers. The network has two linear layers:\n","\n","W1 is the weight matrix of the first (input → hidden) layer (shape: (input_dim, hidden_dim)), and b1 is the bias of the first layer (shape: (1, hidden_dim)), added after x.mm(W1).\n","\n","W2 is the weight matrix of the second (hidden → output) layer (shape: (hidden_dim, output_dim)), and b2 is the bias of the output layer (shape: (1, output_dim)), added after h.mm(W2).\n","\n","To make these parameters learnable, we set requires_grad=True so PyTorch tracks operations involving them and can compute gradients automatically.\n","\n","In each training step we:\n","\n","Zero stored gradients (because gradients accumulate by default).\n","\n","Run a forward pass to compute predictions.\n","\n","Compute a sum of squared errors loss.\n","\n","Call loss.backward() to compute gradients for W1, b1, W2, and b2.\n","\n","Update parameters using gradient descent with a learning rate.\n","\n","The goal is to understand the core mechanics behind training: forward pass → loss → gradients → parameter update, and why we must reset gradients between steps."],"metadata":{"id":"Z2vZoguO14d_"}},{"cell_type":"code","source":["# 1) Device (optional)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)"],"metadata":{"id":"PIniMGjd68nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2) Small training set (5 samples, 2 features) + targets (5 samples, 1 output)\n","training_inputs = torch.tensor(\n","    [[1, 0],\n","     [1, 1],\n","     [0, 0],\n","     [0, 1],\n","     [1, 0]],\n","    dtype=torch.float32,\n","    device=device\n",")\n","\n","training_targets = torch.tensor(\n","    [[1],\n","     [0],\n","     [0],\n","     [1],\n","     [1]],\n","    dtype=torch.float32,\n","    device=device\n",")"],"metadata":{"id":"Mv-utOAW7Aea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3) Network sizes\n","input_dim = 2\n","hidden_dim = 1\n","output_dim = 1"],"metadata":{"id":"PBlJi4KT7Jep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4) Initialize parameters\n","W1 = torch.randn(input_dim, hidden_dim, device=device, requires_grad=True)\n","b1 = torch.randn(1, hidden_dim, device=device, requires_grad=True)\n","\n","W2 = torch.randn(hidden_dim, output_dim, device=device, requires_grad=True)\n","b2 = torch.randn(1, output_dim, device=device, requires_grad=True)\n","\n","learning_rate = 0.01\n","\n","def forward(x: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Compute network output (no activation here, just linear layers).\"\"\"\n","    h = x.mm(W1) + b1          # (N, hidden_dim)\n","    y = h.mm(W2) + b2          # (N, output_dim)\n","    return y\n","\n","def compute_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Sum of squared errors.\"\"\"\n","    losses = (y_pred - y_true) ** 2\n","    loss = losses.sum()\n","    return loss\n","\n","def zero_gradients():\n","    \"\"\"Zero stored gradients to avoid accumulation across steps.\"\"\"\n","    for p in (W1, b1, W2, b2):\n","        if p.grad is not None:\n","            p.grad.zero_()"],"metadata":{"id":"K3CWriNK7QbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- One manual training step ---\n","zero_gradients()\n","\n","y_pred = forward(training_inputs)\n","loss = compute_loss(y_pred, training_targets)\n","print(\"Loss:\", loss.item())\n","\n","loss.backward()  # compute gradients"],"metadata":{"id":"YnixKoaw7Woe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W1"],"metadata":{"id":"P3qftTG0wcKf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradient descent update (manual)\n","with torch.no_grad():\n","    W1 -= learning_rate * W1.grad\n","    b1 -= learning_rate * b1.grad\n","    W2 -= learning_rate * W2.grad\n","    b2 -= learning_rate * b2.grad\n"],"metadata":{"id":"l13o-IdG7aJT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W1"],"metadata":{"id":"lirlX-qHwfXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (Optional) show gradients after backward\n","print(\"W1.grad:\", W1.grad)\n","print(\"b1.grad:\", b1.grad)\n","print(\"W2.grad:\", W2.grad)\n","print(\"b2.grad:\", b2.grad)"],"metadata":{"id":"lpqtjC5V7hwc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 17: One Manual Training Step with Autograd (2-Layer Network)**\n","\n","In this exercise you will perform one full manual training step for a small 2-layer feed-forward network in PyTorch (no torch.nn, no optimizer). The goal is to practice:\n","\n","setting requires_grad=True for learnable parameters,\n","\n","computing a forward pass using mm() and bias addition,\n","\n","computing a loss,\n","\n","calling backward() to get gradients,\n","\n","updating parameters with gradient descent,\n","\n","zeroing gradients to avoid accumulation.\n","\n","Data (use exactly these values):\n","\n","Inputs (4 samples, 2 features):\n","\n","[[1, 0],\n"," [0, 1],\n"," [1, 1],\n"," [0, 0]]\n","\n","\n","Targets (4 samples, 1 output):\n","\n","[[1],\n"," [1],\n"," [0],\n"," [0]]\n","\n","\n","Network sizes:\n","\n","input_dim = 2\n","\n","hidden_dim = 2\n","\n","output_dim = 1\n","\n","Tasks\n","\n","Create training_inputs and training_targets (dtype=torch.float32) on the chosen device.\n","\n","Initialize W1, b1, W2, b2 with random values and set requires_grad=True.\n","\n","Write:\n","\n","forward(x) that computes h = x.mm(W1) + b1 and y_pred = h.mm(W2) + b2\n","\n","compute_loss(y_pred, y_true) that returns the sum of squared errors\n","\n","zero_gradients() that zeros .grad for all parameters if it exists\n","\n","Run one training step in this order:\n","\n","zero_gradients()\n","\n","y_pred = forward(training_inputs)\n","\n","loss = compute_loss(y_pred, training_targets) and print loss.item()\n","\n","loss.backward()\n","\n","update all parameters with learning_rate = 0.05 using with torch.no_grad():\n","\n","Print W1.grad once (to confirm gradients were computed).\n","\n","Theory question (one short sentence):\n","Why must we call zero_gradients() before each new backward pass?"],"metadata":{"id":"AZdZTgyX9Td8"}},{"cell_type":"code","source":[],"metadata":{"id":"zF-04pqB9gHN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 18. This example shows a complete manual training loop for a small neural network in PyTorch. It brings together everything you learned earlier:\n","\n","Forward pass: compute the model output y from the inputs.\n","\n","Loss computation: measure how wrong y is compared to the targets.\n","\n","Backward pass: call loss.backward() to compute gradients (.grad) for the learnable parameters (W1, b1, W2, b2).\n","\n","Parameter update (gradient descent): update each parameter by moving it a small step in the negative gradient direction using a learning rate.\n","\n","Repeat many times: run this cycle for many iterations (e.g., 2000) to gradually reduce the loss.\n","\n","Key idea: training is an iterative process:\n","zero gradients → forward → loss → backward → update → repeat."],"metadata":{"id":"jr-CKLfSAsn9"}},{"cell_type":"code","source":["learning_rate = 0.01"],"metadata":{"id":"wmtFZbsrA-k_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_inputs = torch.tensor(\n","    [[1, 0],\n","     [1, 1],\n","     [0, 0],\n","     [0, 1],\n","     [1, 0]],\n","    dtype=torch.float32,\n","    device=device\n",")\n","\n","training_targets = torch.tensor(\n","    [[1],\n","     [0],\n","     [0],\n","     [1],\n","     [1]],\n","    dtype=torch.float32,\n","    device=device\n",")\n","\n","input_dim = 2\n","hidden_dim = 1\n","output_dim = 1\n","\n","W1 = torch.randn(input_dim, hidden_dim, device=device, requires_grad=True)\n","b1 = torch.randn(1, hidden_dim, device=device, requires_grad=True)\n","\n","W2 = torch.randn(hidden_dim, output_dim, device=device, requires_grad=True)\n","b2 = torch.randn(1, output_dim, device=device, requires_grad=True)\n","\n","learning_rate = 0.01\n","\n","def forward(x: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Compute network output (no activation here, just linear layers).\"\"\"\n","    h = x.mm(W1) + b1          # (N, hidden_dim)\n","    y = h.mm(W2) + b2          # (N, output_dim)\n","    return y\n","\n","def compute_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Sum of squared errors.\"\"\"\n","    losses = (y_pred - y_true) ** 2\n","    loss = losses.sum()\n","    return loss\n","\n","def zero_gradients():\n","    \"\"\"Zero stored gradients to avoid accumulation across steps.\"\"\"\n","    for p in (W1, b1, W2, b2):\n","        if p.grad is not None:\n","            p.grad.zero_()\n","\n","def refresh(params):\n","    # Manual gradient descent update (safe way)\n","    with torch.no_grad():\n","        for p in params:\n","            p -= learning_rate * p.grad\n","\n","for i in range(2000):\n","    zero_gradients()\n","\n","    y = forward(training_inputs)\n","    loss = compute_loss(y, training_targets)\n","\n","\n","    if i % 200 == 0:\n","        print(f\"round {i} - loss {loss}\")\n","\n","    loss.backward()\n","    refresh([W1, b1, W2, b2])\n","\n","print(\"output:\")\n","print(repr(y))\n","print(\"final loss:\", loss.item())"],"metadata":{"id":"_pZ6WsGeBjaX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 18: Train a Tiny Network with a Manual Training Loop (200 Iterations)**\n","\n","In this exercise you will implement a full manual training loop for a small 2-layer network in PyTorch (no torch.nn, no optimizer). The goal is to practice the standard training cycle:\n","\n","zero gradients → forward → loss → backward → update → repeat\n","\n","Data (use exactly these values)\n","\n","Inputs (4 samples, 2 features):\n","\n","[[1, 0],\n"," [0, 1],\n"," [1, 1],\n"," [0, 0]]\n","\n","\n","Targets (4 samples, 1 output):\n","\n","[[1],\n"," [1],\n"," [0],\n"," [0]]\n","\n","Network sizes\n","\n","input_dim = 2\n","\n","hidden_dim = 2\n","\n","output_dim = 1\n","\n","Tasks\n","\n","Choose a device (cuda if available, else cpu).\n","\n","Create training_inputs and training_targets on the device (dtype=torch.float32).\n","\n","Initialize learnable parameters W1, b1, W2, b2 with requires_grad=True.\n","\n","Write:\n","\n","forward(x) that computes h = x.mm(W1) + b1 and y_pred = h.mm(W2) + b2\n","\n","compute_loss(y_pred, y_true) that returns sum of squared errors\n","\n","zero_gradients() that zeros .grad for all parameters\n","\n","refresh() that updates parameters using gradient descent with learning_rate = 0.05\n","\n","Run a training loop for 200 iterations. Print the loss every 50 iterations.\n","\n","After training, print the final predictions y_pred.\n","\n","Theory question (one short sentence):\n","What are the five main steps repeated in each training iteration?"],"metadata":{"id":"uWedKsE3ChPT"}},{"cell_type":"code","source":[],"metadata":{"id":"nwVNfuTJCs9l"},"execution_count":null,"outputs":[]}]}